# =============================================================================
# AI Research Agent - Multi-Provider LLM Configuration
# =============================================================================

# -----------------------------------------------------------------------------
# LLM Provider Configuration
# At least one provider must be configured for the system to work
# -----------------------------------------------------------------------------

# OpenAI Configuration
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=4000

# Anthropic Configuration
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
ANTHROPIC_TEMPERATURE=0.7
ANTHROPIC_MAX_TOKENS=4000

# Google Gemini Configuration
# Get your API key from: https://ai.google.dev/
GOOGLE_API_KEY=your_google_api_key_here
GEMINI_MODEL=gemini-1.5-pro
GEMINI_TEMPERATURE=0.6
GEMINI_MAX_TOKENS=4000

# Ollama Configuration (for local models)
# Make sure Ollama is running: ollama serve
ENABLE_OLLAMA=false
OLLAMA_MODEL=llama3.1
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TEMPERATURE=0.8
OLLAMA_MAX_TOKENS=4000

# -----------------------------------------------------------------------------
# Global Settings
# -----------------------------------------------------------------------------

# Default provider to use when no node-specific provider is set
# Options: openai, anthropic, gemini, ollama
DEFAULT_LLM_PROVIDER=openai

# Global model settings (applied to default provider)
MODEL=gpt-4o
TEMPERATURE=0.7
MAX_TOKENS=4000

# -----------------------------------------------------------------------------
# Node-Specific Provider Configuration
# Override which provider each node uses
# -----------------------------------------------------------------------------

# Example: Use different providers for different tasks
# PLAN_PROVIDER=openai
# WEB_RESEARCH_PROVIDER=openai
# PRIOR_ART_PROVIDER=anthropic
# SYNTHESIZE_PROVIDER=anthropic
# CRITICISM_PROVIDER=gemini
# VALIDATE_PROVIDER=openai
# PERSIST_PROVIDER=openai

# Node-specific model overrides
# SYNTHESIZE_MODEL=claude-3-opus-20240229
# WEB_RESEARCH_MODEL=gpt-4o-mini
# CRITICISM_MODEL=gemini-1.5-pro

# -----------------------------------------------------------------------------
# MCP (Model Context Protocol) Configuration
# -----------------------------------------------------------------------------

# Enable/Disable MCP integration
USE_MCP=true

# GitHub Token (for code search via MCP)
# Get a personal access token from: https://github.com/settings/tokens
GITHUB_TOKEN=your_github_token_here

# Tavily API Key (for web search via MCP)
# Get your API key from: https://tavily.com
TAVILY_API_KEY=your_tavily_api_key_here

# MCP tools available to each node (comma-separated)
# WEB_RESEARCH_MCP_TOOLS=web_search,tavily
# SYNTHESIZE_MCP_TOOLS=web_search,github,filesystem
# CRITICISM_MCP_TOOLS=web_search

# -----------------------------------------------------------------------------
# Example Configurations
# -----------------------------------------------------------------------------

# Cost-optimized setup (use cheaper models for most tasks)
# DEFAULT_LLM_PROVIDER=openai
# MODEL=gpt-4o-mini
# SYNTHESIZE_PROVIDER=openai
# SYNTHESIZE_MODEL=gpt-4o

# Quality-focused setup (use best models)
# DEFAULT_LLM_PROVIDER=anthropic
# SYNTHESIZE_PROVIDER=anthropic
# SYNTHESIZE_MODEL=claude-3-opus-20240229
# CRITICISM_PROVIDER=gemini
# CRITICISM_MODEL=gemini-1.5-pro

# Local-first setup with cloud fallback
# DEFAULT_LLM_PROVIDER=ollama
# SYNTHESIZE_PROVIDER=anthropic  # Use cloud for complex synthesis
# CRITICISM_PROVIDER=openai      # Use cloud for criticism
